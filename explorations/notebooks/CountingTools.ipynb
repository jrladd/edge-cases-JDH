{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Tools\n",
    "## How to Count Words in Python and Pandas\n",
    "\n",
    "This notebook gives a brief introduction to counting words in Python and Pandas. While word counting is the most basic form of text mining, it still involves a number of interpretative steps. To help you understand our choices in our article, we are providing this notebook as a supplementary resource and introduction to these tradeoffs. We are not attempting to be comprehensive here, but rather to give you a sense of the kinds of decisions that go into word counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional resources, see William J. Turkel and Adam Crymble, \"Counting Word Frequencies with Python,\" *Programming Historian* 1 (2012), https://doi.org/10.46430/phen0003 and Megan S. Kane, \"Corpus Analysis with spaCy,\" *Programming Historian* 12 (2023), https://doi.org/10.46430/phen0113."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Relevant Libraries and Create Shared Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/zleblanc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Optional, Tuple\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "## If you haven't downloaded the NLTK data sets yet, do so:\n",
    "def download_nltk_data_if_needed(packages):\n",
    "    for package in packages:\n",
    "        try:\n",
    "            nltk.data.find(package)\n",
    "        except LookupError:\n",
    "            nltk.download(package)\n",
    "\n",
    "download_nltk_data_if_needed(['tokenizers/punkt', 'corpora/stopwords', 'wordnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_cells(val):\n",
    "    \"\"\"\n",
    "    Takes a scalar and returns a string with\n",
    "    the css property `'color: red'` for negative\n",
    "    strings, `'color: green'` for positive strings.\n",
    "    \"\"\"\n",
    "    color = 'red' if val > 0 else 'blue'\n",
    "    return 'color: %s' % color\n",
    "\n",
    "def make_pretty(styler, subset_columns):\n",
    "    styler.applymap(color_cells, subset=subset_columns)\n",
    "    return styler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Example Dataset\n",
    "\n",
    "Given our article's focus on DH tools, we are creating a dataset that explores how we count the word `tool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>digital tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Critical Tool Studies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Footstool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DH TOOLSETS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tooling up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            example_text\n",
       "0          digital tools\n",
       "1  Critical Tool Studies\n",
       "2              Footstool\n",
       "3            DH TOOLSETS\n",
       "4             tooling up"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tool_data = pd.DataFrame({'example_text': ['digital tools', 'Critical Tool Studies', 'Footstool', 'DH TOOLSETS', 'tooling up']})\n",
    "example_tool_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counting Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: String Matching\n",
    "\n",
    "Computers are very good at counting things, but also very literal. The simplest way to count words is to tell the computer to look for the word you want to count. This is called \"string matching.\" In Pandas, we can do this with the `str.count()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_99113_row0_col1, #T_99113_row2_col1, #T_99113_row4_col1 {\n",
       "  color: red;\n",
       "}\n",
       "#T_99113_row1_col1, #T_99113_row3_col1 {\n",
       "  color: blue;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_99113\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_99113_level0_col0\" class=\"col_heading level0 col0\" >example_text</th>\n",
       "      <th id=\"T_99113_level0_col1\" class=\"col_heading level0 col1\" >string_matching</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_99113_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_99113_row0_col0\" class=\"data row0 col0\" >digital tools</td>\n",
       "      <td id=\"T_99113_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_99113_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_99113_row1_col0\" class=\"data row1 col0\" >Critical Tool Studies</td>\n",
       "      <td id=\"T_99113_row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_99113_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_99113_row2_col0\" class=\"data row2 col0\" >Footstool</td>\n",
       "      <td id=\"T_99113_row2_col1\" class=\"data row2 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_99113_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_99113_row3_col0\" class=\"data row3 col0\" >DH TOOLSETS</td>\n",
       "      <td id=\"T_99113_row3_col1\" class=\"data row3 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_99113_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_99113_row4_col0\" class=\"data row4 col0\" >tooling up</td>\n",
       "      <td id=\"T_99113_row4_col1\" class=\"data row4 col1\" >1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x15fa557f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tool_data['string_matching'] = example_tool_data['example_text'].str.count('tool')\n",
    "example_tool_data.style.pipe(make_pretty, subset_columns=['string_matching']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this quick example, we can see that three of our examples are counted correctly, but two are not (`Critical tool Studies` and `DH TOOLSETS`). This is because the `str.count()` method is case sensitive, and those examples are capitalized and uppercase, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_66084_row0_col1, #T_66084_row1_col1, #T_66084_row2_col1, #T_66084_row3_col1, #T_66084_row4_col1 {\n",
       "  color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_66084\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_66084_level0_col0\" class=\"col_heading level0 col0\" >example_text</th>\n",
       "      <th id=\"T_66084_level0_col1\" class=\"col_heading level0 col1\" >string_matching</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_66084_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_66084_row0_col0\" class=\"data row0 col0\" >digital tools</td>\n",
       "      <td id=\"T_66084_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66084_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_66084_row1_col0\" class=\"data row1 col0\" >Critical Tool Studies</td>\n",
       "      <td id=\"T_66084_row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66084_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_66084_row2_col0\" class=\"data row2 col0\" >Footstool</td>\n",
       "      <td id=\"T_66084_row2_col1\" class=\"data row2 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66084_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_66084_row3_col0\" class=\"data row3 col0\" >DH TOOLSETS</td>\n",
       "      <td id=\"T_66084_row3_col1\" class=\"data row3 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66084_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_66084_row4_col0\" class=\"data row4 col0\" >tooling up</td>\n",
       "      <td id=\"T_66084_row4_col1\" class=\"data row4 col1\" >1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x168d83d30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tool_data['string_matching'] = example_tool_data['example_text'].str.count('tool|Tool|TOOL')\n",
    "example_tool_data.style.pipe(make_pretty, subset_columns=['string_matching']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used a simple `OR` pipe operator to count all the versions of tools, which has worked. But we are currently counting `Footstool` a word that contains `tool` but not an instance of `tool`. Consequently, straight string matching is slightly too permissive for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Tokenization\n",
    "\n",
    "Rather than counting strings, we can count words. This is called \"tokenization.\" Tokenization is the process of breaking up a string into smaller units, called tokens. In this case, we want to break up our string into words. We can do this with the `str.split()` method. Tokenization is very language-specific, but since our data is in English, we can use the default settings. For an example of non-English tokenization, see Melanie Walsh, *Introduction to Cultural Analytics & Python*, Version 1 (2021), https://doi.org/10.5281/zenodo.4411250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_64a85_row0_col1, #T_64a85_row1_col1, #T_64a85_row1_col3, #T_64a85_row2_col1, #T_64a85_row3_col1, #T_64a85_row4_col1 {\n",
       "  color: red;\n",
       "}\n",
       "#T_64a85_row0_col3, #T_64a85_row2_col3, #T_64a85_row3_col3, #T_64a85_row4_col3 {\n",
       "  color: blue;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_64a85\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_64a85_level0_col0\" class=\"col_heading level0 col0\" >example_text</th>\n",
       "      <th id=\"T_64a85_level0_col1\" class=\"col_heading level0 col1\" >string_matching</th>\n",
       "      <th id=\"T_64a85_level0_col2\" class=\"col_heading level0 col2\" >tokenized_example_text</th>\n",
       "      <th id=\"T_64a85_level0_col3\" class=\"col_heading level0 col3\" >tokenized_string_matching</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_64a85_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_64a85_row0_col0\" class=\"data row0 col0\" >digital tools</td>\n",
       "      <td id=\"T_64a85_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "      <td id=\"T_64a85_row0_col2\" class=\"data row0 col2\" >['digital', 'tools']</td>\n",
       "      <td id=\"T_64a85_row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_64a85_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_64a85_row1_col0\" class=\"data row1 col0\" >Critical Tool Studies</td>\n",
       "      <td id=\"T_64a85_row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "      <td id=\"T_64a85_row1_col2\" class=\"data row1 col2\" >['Critical', 'Tool', 'Studies']</td>\n",
       "      <td id=\"T_64a85_row1_col3\" class=\"data row1 col3\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_64a85_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_64a85_row2_col0\" class=\"data row2 col0\" >Footstool</td>\n",
       "      <td id=\"T_64a85_row2_col1\" class=\"data row2 col1\" >1</td>\n",
       "      <td id=\"T_64a85_row2_col2\" class=\"data row2 col2\" >['Footstool']</td>\n",
       "      <td id=\"T_64a85_row2_col3\" class=\"data row2 col3\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_64a85_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_64a85_row3_col0\" class=\"data row3 col0\" >DH TOOLSETS</td>\n",
       "      <td id=\"T_64a85_row3_col1\" class=\"data row3 col1\" >1</td>\n",
       "      <td id=\"T_64a85_row3_col2\" class=\"data row3 col2\" >['DH', 'TOOLSETS']</td>\n",
       "      <td id=\"T_64a85_row3_col3\" class=\"data row3 col3\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_64a85_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_64a85_row4_col0\" class=\"data row4 col0\" >tooling up</td>\n",
       "      <td id=\"T_64a85_row4_col1\" class=\"data row4 col1\" >1</td>\n",
       "      <td id=\"T_64a85_row4_col2\" class=\"data row4 col2\" >['tooling', 'up']</td>\n",
       "      <td id=\"T_64a85_row4_col3\" class=\"data row4 col3\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x15fa6a2e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tool_data['tokenized_example_text'] = example_tool_data['example_text'].apply(word_tokenize)\n",
    "example_tool_data['tokenized_string_matching'] = example_tool_data['tokenized_example_text'].apply(lambda x: sum(1 for token in x if token in ['tool', 'Tool', 'TOOL']))\n",
    "example_tool_data.style.pipe(make_pretty, subset_columns=['string_matching', 'tokenized_string_matching']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have tokenized our `example_text` using the `NLTK` library, rather than `str.split(' ')` since we use NLTK in our article (but they are essentially the same here). We can see that in this example we are now only getting the exact match for `tool`, and not any of other examples. This approach is much more restrictive than string matching. We could add `tools` to our list of allowed terms to get `digital tools`, though then we would need to write `tools` and `Tools` and `TOOLS` to be equally comprehensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 3: Lowercasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having to write out all versions of our terms, we can lowercase all of our text to help normalize our data. This will help us avoid the problem of case sensitivity. We can do this with the `str.lower()` method. This type of transformation is often part of pre-processing or data cleaning, but can be enormously impactful on the results of your analysis. However, in our case, we want to capture both `Tool` and `tool`, as well as `Tools` and `tools` so this approach makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_171d0_row0_col1, #T_171d0_row0_col2, #T_171d0_row0_col3, #T_171d0_row0_col4, #T_171d0_row1_col1, #T_171d0_row1_col3, #T_171d0_row1_col4, #T_171d0_row2_col1, #T_171d0_row2_col3, #T_171d0_row3_col1, #T_171d0_row3_col3, #T_171d0_row4_col1, #T_171d0_row4_col3 {\n",
       "  color: red;\n",
       "}\n",
       "#T_171d0_row1_col2, #T_171d0_row2_col2, #T_171d0_row2_col4, #T_171d0_row3_col2, #T_171d0_row3_col4, #T_171d0_row4_col2, #T_171d0_row4_col4 {\n",
       "  color: blue;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_171d0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_171d0_level0_col0\" class=\"col_heading level0 col0\" >example_text</th>\n",
       "      <th id=\"T_171d0_level0_col1\" class=\"col_heading level0 col1\" >string_matching</th>\n",
       "      <th id=\"T_171d0_level0_col2\" class=\"col_heading level0 col2\" >tokenized_string_matching</th>\n",
       "      <th id=\"T_171d0_level0_col3\" class=\"col_heading level0 col3\" >lower_string_matching</th>\n",
       "      <th id=\"T_171d0_level0_col4\" class=\"col_heading level0 col4\" >tokenized_lower_string_matching</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_171d0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_171d0_row0_col0\" class=\"data row0 col0\" >digital tools</td>\n",
       "      <td id=\"T_171d0_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "      <td id=\"T_171d0_row0_col2\" class=\"data row0 col2\" >1</td>\n",
       "      <td id=\"T_171d0_row0_col3\" class=\"data row0 col3\" >1</td>\n",
       "      <td id=\"T_171d0_row0_col4\" class=\"data row0 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_171d0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_171d0_row1_col0\" class=\"data row1 col0\" >Critical Tool Studies</td>\n",
       "      <td id=\"T_171d0_row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "      <td id=\"T_171d0_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "      <td id=\"T_171d0_row1_col3\" class=\"data row1 col3\" >1</td>\n",
       "      <td id=\"T_171d0_row1_col4\" class=\"data row1 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_171d0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_171d0_row2_col0\" class=\"data row2 col0\" >Footstool</td>\n",
       "      <td id=\"T_171d0_row2_col1\" class=\"data row2 col1\" >1</td>\n",
       "      <td id=\"T_171d0_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "      <td id=\"T_171d0_row2_col3\" class=\"data row2 col3\" >1</td>\n",
       "      <td id=\"T_171d0_row2_col4\" class=\"data row2 col4\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_171d0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_171d0_row3_col0\" class=\"data row3 col0\" >DH TOOLSETS</td>\n",
       "      <td id=\"T_171d0_row3_col1\" class=\"data row3 col1\" >1</td>\n",
       "      <td id=\"T_171d0_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "      <td id=\"T_171d0_row3_col3\" class=\"data row3 col3\" >1</td>\n",
       "      <td id=\"T_171d0_row3_col4\" class=\"data row3 col4\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_171d0_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_171d0_row4_col0\" class=\"data row4 col0\" >tooling up</td>\n",
       "      <td id=\"T_171d0_row4_col1\" class=\"data row4 col1\" >1</td>\n",
       "      <td id=\"T_171d0_row4_col2\" class=\"data row4 col2\" >0</td>\n",
       "      <td id=\"T_171d0_row4_col3\" class=\"data row4 col3\" >1</td>\n",
       "      <td id=\"T_171d0_row4_col4\" class=\"data row4 col4\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x15fa6ac40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tool_data['lower_example_text'] = example_tool_data['example_text'].str.lower()\n",
    "example_tool_data['lower_string_matching'] = example_tool_data['lower_example_text'].str.count('tool|Tool|TOOL')\n",
    "example_tool_data['tokenized_lower_example_text'] = example_tool_data['lower_example_text'].apply(word_tokenize)\n",
    "example_tool_data['tokenized_string_matching'] = example_tool_data['tokenized_example_text'].apply(lambda x: sum(1 for token in x if token in ['tool', 'tools']))\n",
    "example_tool_data['tokenized_lower_string_matching'] = example_tool_data['tokenized_lower_example_text'].apply(lambda x: sum(1 for token in x if token in ['tool', 'tools']))\n",
    "example_tool_data[['example_text', 'string_matching', 'tokenized_string_matching', 'lower_string_matching', 'tokenized_lower_string_matching']].style.pipe(make_pretty, subset_columns=['string_matching', 'tokenized_string_matching', 'lower_string_matching', 'tokenized_lower_string_matching']) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our tokenization, this time we are searching for both `tool` and `tools`, but we've also tried lowercasing our data. While we can see that the lowercased text is equally as permissive on string matching, but with tokenization we finally get our two instances of `tool` that we want to include. This is because we are now searching for `tool` and `tools` in our tokenized text, rather than just `tool`. We are not getting `Footstool` anymore, but we are also not getting `TOOLSETS` or `tooling`. In our article, we have decided to be a bit more restrictive and just focus on those most obvious instances of tool, but there are methods to get more of these examples if you want to be more inclusive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 4: Lemmatization & Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main other methods for normalizing textual data are lemmatizing and stemming. Lemmatizing and stemming are both methods of reducing words to their root form. Lemmatizing is more sophisticated than stemming, but both are useful for reducing the number of unique words in your dataset. For example, `tools` and `tool` would both be reduced to `tool`. Whereas stemming is a bit more aggressive and would also lower case the word, so `Tools` would also be reduced to `tool`. We can do this with the `nltk.stem` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define functions for stemming and lemmatization\n",
    "def stem_text(tokens):\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "def lemmatize_text(tokens):\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>digital tools</td>\n",
       "      <td>digit tool</td>\n",
       "      <td>digital tool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Critical Tool Studies</td>\n",
       "      <td>critic tool studi</td>\n",
       "      <td>Critical Tool Studies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Footstool</td>\n",
       "      <td>footstool</td>\n",
       "      <td>Footstool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DH TOOLSETS</td>\n",
       "      <td>dh toolset</td>\n",
       "      <td>DH TOOLSETS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tooling up</td>\n",
       "      <td>tool up</td>\n",
       "      <td>tooling up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            example_text       stemmed_text        lemmatized_text\n",
       "0          digital tools         digit tool           digital tool\n",
       "1  Critical Tool Studies  critic tool studi  Critical Tool Studies\n",
       "2              Footstool          footstool              Footstool\n",
       "3            DH TOOLSETS         dh toolset            DH TOOLSETS\n",
       "4             tooling up            tool up             tooling up"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the functions to the 'example_text' column\n",
    "example_tool_data['stemmed_text'] = example_tool_data['tokenized_example_text'].apply(stem_text)\n",
    "example_tool_data['lemmatized_text'] = example_tool_data['tokenized_example_text'].apply(lemmatize_text)\n",
    "example_tool_data[['example_text', 'stemmed_text', 'lemmatized_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that `stemming` took terms like `digital tools` and turned them into `digit tool`, or in the case of `Critical Tool Studies` it turned it into `critic tool studi`. However, we aren't seeing much changes with the lemmatizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>digital tools</td>\n",
       "      <td>digit tool</td>\n",
       "      <td>digital tool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Critical Tool Studies</td>\n",
       "      <td>critic tool studi</td>\n",
       "      <td>critical tool study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Footstool</td>\n",
       "      <td>footstool</td>\n",
       "      <td>footstool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DH TOOLSETS</td>\n",
       "      <td>dh toolset</td>\n",
       "      <td>dh toolsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tooling up</td>\n",
       "      <td>tool up</td>\n",
       "      <td>tooling up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            example_text       stemmed_text      lemmatized_text\n",
       "0          digital tools         digit tool         digital tool\n",
       "1  Critical Tool Studies  critic tool studi  critical tool study\n",
       "2              Footstool          footstool            footstool\n",
       "3            DH TOOLSETS         dh toolset          dh toolsets\n",
       "4             tooling up            tool up           tooling up"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the functions to the 'example_text' column\n",
    "example_tool_data['stemmed_text'] = example_tool_data['tokenized_lower_example_text'].apply(stem_text)\n",
    "example_tool_data['lemmatized_text'] = example_tool_data['tokenized_lower_example_text'].apply(lemmatize_text)\n",
    "example_tool_data[['example_text', 'stemmed_text', 'lemmatized_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are running both methods on our `lowercased` and `tokenized` data. This time `lemmatizing` is turning `tools` into `tool`, and `studies` into `study`. Whereas `stemming` is not only transforming those terms, but also turning `toolsets` into `toolset` and `tooling` into `tool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b31a9_row0_col1, #T_b31a9_row0_col2, #T_b31a9_row0_col3, #T_b31a9_row0_col4, #T_b31a9_row1_col2, #T_b31a9_row1_col3, #T_b31a9_row1_col4, #T_b31a9_row4_col4 {\n",
       "  color: red;\n",
       "}\n",
       "#T_b31a9_row1_col1, #T_b31a9_row2_col1, #T_b31a9_row2_col2, #T_b31a9_row2_col3, #T_b31a9_row2_col4, #T_b31a9_row3_col1, #T_b31a9_row3_col2, #T_b31a9_row3_col3, #T_b31a9_row3_col4, #T_b31a9_row4_col1, #T_b31a9_row4_col2, #T_b31a9_row4_col3 {\n",
       "  color: blue;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b31a9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b31a9_level0_col0\" class=\"col_heading level0 col0\" >example_text</th>\n",
       "      <th id=\"T_b31a9_level0_col1\" class=\"col_heading level0 col1\" >tokenized_string_matching</th>\n",
       "      <th id=\"T_b31a9_level0_col2\" class=\"col_heading level0 col2\" >tokenized_lower_string_matching</th>\n",
       "      <th id=\"T_b31a9_level0_col3\" class=\"col_heading level0 col3\" >tokenized_lemmatized_string_matching</th>\n",
       "      <th id=\"T_b31a9_level0_col4\" class=\"col_heading level0 col4\" >tokenized_stemmed_string_matching</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b31a9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b31a9_row0_col0\" class=\"data row0 col0\" >digital tools</td>\n",
       "      <td id=\"T_b31a9_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "      <td id=\"T_b31a9_row0_col2\" class=\"data row0 col2\" >1</td>\n",
       "      <td id=\"T_b31a9_row0_col3\" class=\"data row0 col3\" >1</td>\n",
       "      <td id=\"T_b31a9_row0_col4\" class=\"data row0 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b31a9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b31a9_row1_col0\" class=\"data row1 col0\" >Critical Tool Studies</td>\n",
       "      <td id=\"T_b31a9_row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "      <td id=\"T_b31a9_row1_col2\" class=\"data row1 col2\" >1</td>\n",
       "      <td id=\"T_b31a9_row1_col3\" class=\"data row1 col3\" >1</td>\n",
       "      <td id=\"T_b31a9_row1_col4\" class=\"data row1 col4\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b31a9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_b31a9_row2_col0\" class=\"data row2 col0\" >Footstool</td>\n",
       "      <td id=\"T_b31a9_row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "      <td id=\"T_b31a9_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "      <td id=\"T_b31a9_row2_col3\" class=\"data row2 col3\" >0</td>\n",
       "      <td id=\"T_b31a9_row2_col4\" class=\"data row2 col4\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b31a9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_b31a9_row3_col0\" class=\"data row3 col0\" >DH TOOLSETS</td>\n",
       "      <td id=\"T_b31a9_row3_col1\" class=\"data row3 col1\" >0</td>\n",
       "      <td id=\"T_b31a9_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "      <td id=\"T_b31a9_row3_col3\" class=\"data row3 col3\" >0</td>\n",
       "      <td id=\"T_b31a9_row3_col4\" class=\"data row3 col4\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b31a9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_b31a9_row4_col0\" class=\"data row4 col0\" >tooling up</td>\n",
       "      <td id=\"T_b31a9_row4_col1\" class=\"data row4 col1\" >0</td>\n",
       "      <td id=\"T_b31a9_row4_col2\" class=\"data row4 col2\" >0</td>\n",
       "      <td id=\"T_b31a9_row4_col3\" class=\"data row4 col3\" >0</td>\n",
       "      <td id=\"T_b31a9_row4_col4\" class=\"data row4 col4\" >1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1684429d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tool_data['tokenized_lemmatized_text'] = example_tool_data['lemmatized_text'].apply(word_tokenize)\n",
    "example_tool_data['tokenized_lemmatized_string_matching'] = example_tool_data['tokenized_lemmatized_text'].apply(lambda x: sum(1 for token in x if token in ['tool', 'tools']))\n",
    "example_tool_data['tokenized_stemmed_text'] = example_tool_data['stemmed_text'].apply(word_tokenize)\n",
    "example_tool_data['tokenized_stemmed_string_matching'] = example_tool_data['tokenized_stemmed_text'].apply(lambda x: sum(1 for token in x if token in ['tool', 'tools']))\n",
    "example_tool_data[['example_text', 'tokenized_string_matching', 'tokenized_lower_string_matching', 'tokenized_lemmatized_string_matching', 'tokenized_stemmed_string_matching']].style.pipe(make_pretty, subset_columns=['tokenized_string_matching', 'tokenized_lower_string_matching','tokenized_lemmatized_string_matching', 'tokenized_stemmed_string_matching'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that if we rerun our tokenization and string matching code, `lemmatization` gets us similar results to simply lowercasing our data. Whereas `stemming` also gets the example of `tooling` that we were missing before. While we could use `stemming` in our article, we have decided to primarily use `lowercasing` and `tokenization`, along with `string matching` to balance both inclusivity and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 5: Our Article's Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final transformation in our article is to not only count words, but to normalize those counts based on the length of their respective document. This helps us know if a term like `tool` is appearing more frequently because it is a longer document, or because it is actually more frequent. We can do this with the `str.len()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_fd9e0_row0_col1, #T_fd9e0_row0_col2, #T_fd9e0_row1_col1, #T_fd9e0_row1_col2, #T_fd9e0_row2_col1, #T_fd9e0_row2_col2, #T_fd9e0_row3_col1, #T_fd9e0_row3_col2, #T_fd9e0_row4_col1, #T_fd9e0_row4_col2 {\n",
       "  color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_fd9e0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fd9e0_level0_col0\" class=\"col_heading level0 col0\" >example_text</th>\n",
       "      <th id=\"T_fd9e0_level0_col1\" class=\"col_heading level0 col1\" >total_length</th>\n",
       "      <th id=\"T_fd9e0_level0_col2\" class=\"col_heading level0 col2\" >total_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_fd9e0_row0_col0\" class=\"data row0 col0\" >digital tools</td>\n",
       "      <td id=\"T_fd9e0_row0_col1\" class=\"data row0 col1\" >13</td>\n",
       "      <td id=\"T_fd9e0_row0_col2\" class=\"data row0 col2\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_fd9e0_row1_col0\" class=\"data row1 col0\" >Critical Tool Studies</td>\n",
       "      <td id=\"T_fd9e0_row1_col1\" class=\"data row1 col1\" >21</td>\n",
       "      <td id=\"T_fd9e0_row1_col2\" class=\"data row1 col2\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_fd9e0_row2_col0\" class=\"data row2 col0\" >Footstool</td>\n",
       "      <td id=\"T_fd9e0_row2_col1\" class=\"data row2 col1\" >9</td>\n",
       "      <td id=\"T_fd9e0_row2_col2\" class=\"data row2 col2\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_fd9e0_row3_col0\" class=\"data row3 col0\" >DH TOOLSETS</td>\n",
       "      <td id=\"T_fd9e0_row3_col1\" class=\"data row3 col1\" >11</td>\n",
       "      <td id=\"T_fd9e0_row3_col2\" class=\"data row3 col2\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fd9e0_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_fd9e0_row4_col0\" class=\"data row4 col0\" >tooling up</td>\n",
       "      <td id=\"T_fd9e0_row4_col1\" class=\"data row4 col1\" >10</td>\n",
       "      <td id=\"T_fd9e0_row4_col2\" class=\"data row4 col2\" >2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x168d83fd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tool_data['total_length'] = example_tool_data['example_text'].apply(len)\n",
    "example_tool_data['total_words'] = example_tool_data['tokenized_example_text'].apply(len)\n",
    "example_tool_data[['example_text', 'total_length', 'total_words']].style.pipe(make_pretty, subset_columns=['total_length', 'total_words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that counting simply characters versus words gives us very different results. In our article, we have primarily counted words (or `tokens`) though again this approach is not perfect for every language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_54eb1_row0_col1, #T_54eb1_row0_col2, #T_54eb1_row0_col3, #T_54eb1_row0_col4, #T_54eb1_row1_col1, #T_54eb1_row1_col2, #T_54eb1_row1_col3, #T_54eb1_row1_col4, #T_54eb1_row2_col2, #T_54eb1_row3_col2, #T_54eb1_row4_col2 {\n",
       "  color: red;\n",
       "}\n",
       "#T_54eb1_row2_col1, #T_54eb1_row2_col3, #T_54eb1_row2_col4, #T_54eb1_row3_col1, #T_54eb1_row3_col3, #T_54eb1_row3_col4, #T_54eb1_row4_col1, #T_54eb1_row4_col3, #T_54eb1_row4_col4 {\n",
       "  color: blue;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_54eb1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_54eb1_level0_col0\" class=\"col_heading level0 col0\" >example_text</th>\n",
       "      <th id=\"T_54eb1_level0_col1\" class=\"col_heading level0 col1\" >tokenized_lower_string_matching</th>\n",
       "      <th id=\"T_54eb1_level0_col2\" class=\"col_heading level0 col2\" >total_words</th>\n",
       "      <th id=\"T_54eb1_level0_col3\" class=\"col_heading level0 col3\" >scaled_tokenized_lower_string_matching</th>\n",
       "      <th id=\"T_54eb1_level0_col4\" class=\"col_heading level0 col4\" >scaled_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_54eb1_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_54eb1_row0_col0\" class=\"data row0 col0\" >digital tools</td>\n",
       "      <td id=\"T_54eb1_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "      <td id=\"T_54eb1_row0_col2\" class=\"data row0 col2\" >2</td>\n",
       "      <td id=\"T_54eb1_row0_col3\" class=\"data row0 col3\" >0.500000</td>\n",
       "      <td id=\"T_54eb1_row0_col4\" class=\"data row0 col4\" >50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_54eb1_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_54eb1_row1_col0\" class=\"data row1 col0\" >Critical Tool Studies</td>\n",
       "      <td id=\"T_54eb1_row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "      <td id=\"T_54eb1_row1_col2\" class=\"data row1 col2\" >3</td>\n",
       "      <td id=\"T_54eb1_row1_col3\" class=\"data row1 col3\" >0.333333</td>\n",
       "      <td id=\"T_54eb1_row1_col4\" class=\"data row1 col4\" >33.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_54eb1_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_54eb1_row2_col0\" class=\"data row2 col0\" >Footstool</td>\n",
       "      <td id=\"T_54eb1_row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "      <td id=\"T_54eb1_row2_col2\" class=\"data row2 col2\" >1</td>\n",
       "      <td id=\"T_54eb1_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n",
       "      <td id=\"T_54eb1_row2_col4\" class=\"data row2 col4\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_54eb1_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_54eb1_row3_col0\" class=\"data row3 col0\" >DH TOOLSETS</td>\n",
       "      <td id=\"T_54eb1_row3_col1\" class=\"data row3 col1\" >0</td>\n",
       "      <td id=\"T_54eb1_row3_col2\" class=\"data row3 col2\" >2</td>\n",
       "      <td id=\"T_54eb1_row3_col3\" class=\"data row3 col3\" >0.000000</td>\n",
       "      <td id=\"T_54eb1_row3_col4\" class=\"data row3 col4\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_54eb1_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_54eb1_row4_col0\" class=\"data row4 col0\" >tooling up</td>\n",
       "      <td id=\"T_54eb1_row4_col1\" class=\"data row4 col1\" >0</td>\n",
       "      <td id=\"T_54eb1_row4_col2\" class=\"data row4 col2\" >2</td>\n",
       "      <td id=\"T_54eb1_row4_col3\" class=\"data row4 col3\" >0.000000</td>\n",
       "      <td id=\"T_54eb1_row4_col4\" class=\"data row4 col4\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x168663ac0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tool_data['scaled_tokenized_lower_string_matching'] = example_tool_data['tokenized_lower_string_matching'] / example_tool_data['total_words']\n",
    "example_tool_data['scaled_percent'] = example_tool_data['scaled_tokenized_lower_string_matching'] * 100\n",
    "\n",
    "example_tool_data[['example_text', 'tokenized_lower_string_matching', 'total_words', 'scaled_tokenized_lower_string_matching', 'scaled_percent']].style.pipe(make_pretty, subset_columns=['tokenized_lower_string_matching', 'total_words', 'scaled_tokenized_lower_string_matching', 'scaled_percent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our final results, which we have used in our article. We have decided to use `lowercasing` and `tokenization` to get our words, and then `string matching` to get our counts. We have also decided to normalize our counts by the length of the document and then finally we have turned those scaled results (which are very small) into percentages. This helps us compare across documents and see which terms are most frequent in each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show an actual example, below is some code from our explorations notebook `ToolsInJournalsEDA.ipynb` and our article `article-text.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def get_term_frequencies(counter: Dict[str, int], terms: List[str], total_tokens: int, lowercase: bool) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate the actual and scaled frequencies of specific terms in a corpus.\n",
    "\n",
    "    Parameters:\n",
    "    counter (Dict[str, int]): A dictionary where the keys are terms (words) and the values are their counts in the corpus.\n",
    "    terms (List[str]): A list of terms for which to calculate frequencies.\n",
    "    total_tokens (int): The total number of tokens (words) in the corpus.\n",
    "    lowercase (bool): Whether to lowercase the terms before calculating frequencies.\n",
    "\n",
    "    Returns:\n",
    "    actual_counts (Dict[str, int]): A dictionary where the keys are the terms and the values are their actual counts in the corpus.\n",
    "    scaled_counts (Dict[str, float]): A dictionary where the keys are the terms and the values are their frequencies in the corpus, scaled by the total number of tokens.\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        actual_counts = {term: counter.get(term.lower(), 0) for term in terms}\n",
    "        scaled_counts = {term: counter.get(term.lower(), 0) / total_tokens for term in terms}\n",
    "    else:\n",
    "        actual_counts = {term: counter.get(term, 0) for term in terms}\n",
    "        scaled_counts = {term: counter.get(term, 0) / total_tokens for term in terms}\n",
    "    return actual_counts, scaled_counts\n",
    "\n",
    "def get_counts(count_df: pd.DataFrame, terms_list: List[str], list_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the actual and scaled frequencies of specific terms in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    count_df (pd.DataFrame): A DataFrame containing token frequencies.\n",
    "    terms_list (List[str]): A list of terms for which to calculate frequencies.\n",
    "    list_name (str): A string to be used in naming the output columns.\n",
    "\n",
    "    Returns:\n",
    "    count_df (pd.DataFrame): The input DataFrame, with additional columns for the actual and scaled frequencies of the terms.\n",
    "    \"\"\"\n",
    "    count_df['lower_' + list_name + '_frequencies'], count_df['scaled_lower_' + list_name + '_frequencies'] = zip(*count_df.apply(lambda x: get_term_frequencies(x['lower_token_frequencies'], terms_list, x['total_tokens'], True), axis=1))\n",
    "    \n",
    "    count_df[list_name + '_term_frequencies'], count_df['scaled_' + list_name + '_term_frequencies'] = zip(*count_df.apply(lambda x: get_term_frequencies(x['token_frequencies'], terms_list, x['total_tokens'], False), axis=1))\n",
    "    \n",
    "    return count_df\n",
    "\n",
    "def get_frequencies(count_df: pd.DataFrame, list_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the frequencies of terms in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    count_df (pd.DataFrame): A DataFrame containing term frequencies.\n",
    "    list_name (str): A string to be used in naming the output columns.\n",
    "\n",
    "    Returns:\n",
    "    merged_df (pd.DataFrame): A DataFrame containing the terms and their frequencies.\n",
    "    \"\"\"\n",
    "    lower_frequencies = Counter()\n",
    "    for freqs in count_df['lower_' + list_name + '_frequencies']:\n",
    "        lower_frequencies.update(freqs)\n",
    "    lower_freq_df = pd.DataFrame(list(lower_frequencies.items()), columns=['Term', 'Frequency_lower'])\n",
    "    \n",
    "    frequencies = Counter()\n",
    "    for freqs in count_df[list_name + '_term_frequencies']:\n",
    "        frequencies.update(freqs)\n",
    "    freq_df = pd.DataFrame(list(frequencies.items()), columns=['Term', 'Frequency'])\n",
    "    \n",
    "    merged_df = pd.merge(lower_freq_df, freq_df, on='Term')\n",
    "    return merged_df\n",
    "\n",
    "def process_dataframe(df: pd.DataFrame, text_column: str, terms_list: List[str], list_name: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process a DataFrame to calculate term frequencies and tokenize text.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    text_column (str): The name of the column in df that contains the text to process.\n",
    "    terms_list (List[str]): A list of terms for which to calculate frequencies.\n",
    "    list_name (str): A string to be used in naming the output columns.\n",
    "\n",
    "    Returns:\n",
    "    df (pd.DataFrame): The original DataFrame, with additional columns for the lowercased text, tokenized text, token frequencies, and total tokens.\n",
    "    count_df (pd.DataFrame): A copy of df, with additional columns for the actual and scaled frequencies of the terms in terms_list.\n",
    "    terms_df (pd.DataFrame): A DataFrame containing the frequencies of the terms in terms_list.\n",
    "    \"\"\"\n",
    "    if 'lower_text' not in df.columns:\n",
    "        df['lower_text'] = df[text_column].str.lower()\n",
    "\n",
    "    if 'tokenized_text' not in df.columns:\n",
    "        df['tokenized_text'] = df[text_column].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "    if 'tokenized_lower_text' not in df.columns:\n",
    "        df['tokenized_lower_text'] = df['lower_text'].apply(lambda x: word_tokenize(x.lower()))\n",
    "    \n",
    "    if 'lower_token_frequencies' not in df.columns:\n",
    "        df['lower_token_frequencies'] = df['tokenized_lower_text'].apply(lambda x: Counter(x))\n",
    "\n",
    "    if 'token_frequencies' not in df.columns:\n",
    "        df['token_frequencies'] = df['tokenized_text'].apply(lambda x: Counter(x))\n",
    "\n",
    "    if 'total_tokens' not in df.columns:\n",
    "        df['total_tokens'] = df['tokenized_text'].apply(len)\n",
    "    \n",
    "    count_df = df.copy()\n",
    "    count_df = get_counts(count_df, terms_list, list_name)\n",
    "    \n",
    "    terms_df = get_frequencies(count_df, list_name)\n",
    "\n",
    "    return df, count_df, terms_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Create a list of all primariy network analysis tools\n",
    "network_tools = ['Gephi', 'Palladio', 'nodegoat', 'igraph', 'Textexture', 'Netlytic', 'sigma.js', 'Neo4j', 'NetworkX', 'NodeXL', 'Graphviz', 'Cytoscape']\n",
    "# Load the CSV file into a DataFrame\n",
    "index_conferences_df = pd.read_csv(f\"../datasets/dh_conferences_works.csv\")\n",
    "\n",
    "# Create a subset of the DataFrame that only includes rows where 'full_text' is not null\n",
    "subset_index_conferences_df = index_conferences_df[index_conferences_df.full_text.notna()]\n",
    "\n",
    "# Create a 'cleaned_conference_year' column by converting the 'conference_year' column to string and appending \"-01-01\"\n",
    "subset_index_conferences_df['cleaned_conference_year'] = subset_index_conferences_df.conference_year.astype(str) + \"-01-01\"\n",
    "\n",
    "# Convert the 'cleaned_conference_year' column to datetime format\n",
    "subset_index_conferences_df['cleaned_conference_year'] = pd.to_datetime(subset_index_conferences_df['cleaned_conference_year'])\n",
    "\n",
    "# Define the text column, date column, and tools list\n",
    "text_column = 'full_text'\n",
    "date_column = 'cleaned_conference_year'\n",
    "tools_list = network_tools\n",
    "\n",
    "# Process the DataFrame to calculate term frequencies and tokenize text\n",
    "cleaned_index_conferences_df, count_index_conferences_df, tools_index_conferences_df = process_dataframe(subset_index_conferences_df, text_column, tools_list, 'tools')\n",
    "\n",
    "tools_index_conferences_df['Delta_Frequency_Methods'] = tools_index_conferences_df['Frequency_lower'] - tools_index_conferences_df['Frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f041e_row0_col1, #T_f041e_row0_col2, #T_f041e_row0_col3, #T_f041e_row1_col1, #T_f041e_row1_col2, #T_f041e_row2_col1, #T_f041e_row2_col2, #T_f041e_row2_col3, #T_f041e_row3_col1, #T_f041e_row3_col2, #T_f041e_row3_col3, #T_f041e_row4_col1, #T_f041e_row4_col2, #T_f041e_row4_col3, #T_f041e_row6_col1, #T_f041e_row6_col2, #T_f041e_row6_col3, #T_f041e_row7_col1, #T_f041e_row7_col2, #T_f041e_row7_col3, #T_f041e_row8_col1, #T_f041e_row8_col2, #T_f041e_row8_col3, #T_f041e_row9_col1, #T_f041e_row9_col2, #T_f041e_row10_col1, #T_f041e_row10_col2, #T_f041e_row10_col3, #T_f041e_row11_col1, #T_f041e_row11_col2 {\n",
       "  color: red;\n",
       "}\n",
       "#T_f041e_row1_col3, #T_f041e_row5_col1, #T_f041e_row5_col2, #T_f041e_row5_col3, #T_f041e_row9_col3, #T_f041e_row11_col3 {\n",
       "  color: blue;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f041e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f041e_level0_col0\" class=\"col_heading level0 col0\" >Term</th>\n",
       "      <th id=\"T_f041e_level0_col1\" class=\"col_heading level0 col1\" >Frequency_lower</th>\n",
       "      <th id=\"T_f041e_level0_col2\" class=\"col_heading level0 col2\" >Frequency</th>\n",
       "      <th id=\"T_f041e_level0_col3\" class=\"col_heading level0 col3\" >Delta_Frequency_Methods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f041e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f041e_row0_col0\" class=\"data row0 col0\" >Gephi</td>\n",
       "      <td id=\"T_f041e_row0_col1\" class=\"data row0 col1\" >250</td>\n",
       "      <td id=\"T_f041e_row0_col2\" class=\"data row0 col2\" >242</td>\n",
       "      <td id=\"T_f041e_row0_col3\" class=\"data row0 col3\" >8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f041e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f041e_row1_col0\" class=\"data row1 col0\" >Palladio</td>\n",
       "      <td id=\"T_f041e_row1_col1\" class=\"data row1 col1\" >28</td>\n",
       "      <td id=\"T_f041e_row1_col2\" class=\"data row1 col2\" >28</td>\n",
       "      <td id=\"T_f041e_row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f041e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f041e_row2_col0\" class=\"data row2 col0\" >nodegoat</td>\n",
       "      <td id=\"T_f041e_row2_col1\" class=\"data row2 col1\" >60</td>\n",
       "      <td id=\"T_f041e_row2_col2\" class=\"data row2 col2\" >45</td>\n",
       "      <td id=\"T_f041e_row2_col3\" class=\"data row2 col3\" >15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f041e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f041e_row3_col0\" class=\"data row3 col0\" >igraph</td>\n",
       "      <td id=\"T_f041e_row3_col1\" class=\"data row3 col1\" >9</td>\n",
       "      <td id=\"T_f041e_row3_col2\" class=\"data row3 col2\" >6</td>\n",
       "      <td id=\"T_f041e_row3_col3\" class=\"data row3 col3\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f041e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f041e_row4_col0\" class=\"data row4 col0\" >Textexture</td>\n",
       "      <td id=\"T_f041e_row4_col1\" class=\"data row4 col1\" >5</td>\n",
       "      <td id=\"T_f041e_row4_col2\" class=\"data row4 col2\" >2</td>\n",
       "      <td id=\"T_f041e_row4_col3\" class=\"data row4 col3\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f041e_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_f041e_row5_col0\" class=\"data row5 col0\" >Netlytic</td>\n",
       "      <td id=\"T_f041e_row5_col1\" class=\"data row5 col1\" >0</td>\n",
       "      <td id=\"T_f041e_row5_col2\" class=\"data row5 col2\" >0</td>\n",
       "      <td id=\"T_f041e_row5_col3\" class=\"data row5 col3\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f041e_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_f041e_row6_col0\" class=\"data row6 col0\" >sigma.js</td>\n",
       "      <td id=\"T_f041e_row6_col1\" class=\"data row6 col1\" >4</td>\n",
       "      <td id=\"T_f041e_row6_col2\" class=\"data row6 col2\" >1</td>\n",
       "      <td id=\"T_f041e_row6_col3\" class=\"data row6 col3\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f041e_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_f041e_row7_col0\" class=\"data row7 col0\" >Neo4j</td>\n",
       "      <td id=\"T_f041e_row7_col1\" class=\"data row7 col1\" >70</td>\n",
       "      <td id=\"T_f041e_row7_col2\" class=\"data row7 col2\" >28</td>\n",
       "      <td id=\"T_f041e_row7_col3\" class=\"data row7 col3\" >42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f041e_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_f041e_row8_col0\" class=\"data row8 col0\" >NetworkX</td>\n",
       "      <td id=\"T_f041e_row8_col1\" class=\"data row8 col1\" >15</td>\n",
       "      <td id=\"T_f041e_row8_col2\" class=\"data row8 col2\" >12</td>\n",
       "      <td id=\"T_f041e_row8_col3\" class=\"data row8 col3\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f041e_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_f041e_row9_col0\" class=\"data row9 col0\" >NodeXL</td>\n",
       "      <td id=\"T_f041e_row9_col1\" class=\"data row9 col1\" >13</td>\n",
       "      <td id=\"T_f041e_row9_col2\" class=\"data row9 col2\" >13</td>\n",
       "      <td id=\"T_f041e_row9_col3\" class=\"data row9 col3\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f041e_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_f041e_row10_col0\" class=\"data row10 col0\" >Graphviz</td>\n",
       "      <td id=\"T_f041e_row10_col1\" class=\"data row10 col1\" >7</td>\n",
       "      <td id=\"T_f041e_row10_col2\" class=\"data row10 col2\" >1</td>\n",
       "      <td id=\"T_f041e_row10_col3\" class=\"data row10 col3\" >6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f041e_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_f041e_row11_col0\" class=\"data row11 col0\" >Cytoscape</td>\n",
       "      <td id=\"T_f041e_row11_col1\" class=\"data row11 col1\" >13</td>\n",
       "      <td id=\"T_f041e_row11_col2\" class=\"data row11 col2\" >13</td>\n",
       "      <td id=\"T_f041e_row11_col3\" class=\"data row11 col3\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16a21ac70>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools_index_conferences_df.style.pipe(make_pretty, subset_columns=['Frequency_lower', 'Frequency', 'Delta_Frequency_Methods'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the impact of different counting approaches in our *Index of DH Conferences* dataset, with the `Frequency_Lower` showing the *lowercased*, *tokenized*, *string matched* results, and `Frequency` showing the *tokenized*, *string matched* results.\n",
    "\n",
    "We have also included the `Delta_Frequency_Methods` to show how lowercasing leads to more matches, and tokenization leads to fewer matches. These results largely make sense, though noticeably there is a big jump with terms like `Neo4j` and `nodegoat`, likely indicating that these terms are often capitalized in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we used the above code in our initial exploratory data analysis, we used a slightly different approach in the article to deal with bigrams (that is terms that are more than one word) and for processing speed. We have included that code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_term_frequencies(df: pd.DataFrame, text_column: str, term_list: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Vectorize the text and count the frequencies of specific terms.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    text_column (str): The name of the column in df that contains the text to process.\n",
    "    term_list (List[str]): A list of terms for which to calculate frequencies.\n",
    "\n",
    "    Returns:\n",
    "    term_freq_df (pd.DataFrame): A DataFrame where each column corresponds to a term in term_list and the values are the frequencies of the term in each document.\n",
    "    \"\"\"\n",
    "    # Initialize CountVectorizer with the provided terms as the vocabulary\n",
    "    count_vec = CountVectorizer(ngram_range=(1,2), vocabulary=term_list)\n",
    "\n",
    "    # Fit and transform the text data\n",
    "    X = count_vec.fit_transform(df[text_column]).toarray()\n",
    "\n",
    "    # Create a DataFrame with the term frequencies\n",
    "    term_freq_df = pd.DataFrame(X, columns=count_vec.get_feature_names_out())\n",
    "\n",
    "    # For terms that contain a period, use straight string matching\n",
    "    for term in term_list:\n",
    "        if '.' in term:\n",
    "            term_freq_df[term] = df[text_column].str.count(term)\n",
    "\n",
    "    return term_freq_df\n",
    "\n",
    "\n",
    "def generate_word_counts(subset_df: pd.DataFrame, text_column: str, terms_list: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process a DataFrame to calculate term frequencies and tokenize text.\n",
    "\n",
    "    Parameters:\n",
    "    subset_df (pd.DataFrame): The input DataFrame.\n",
    "    text_column (str): The name of the column in subset_df that contains the text to process.\n",
    "    terms_list (List[str]): A list of terms for which to calculate frequencies.\n",
    "    list_name (str): A string to be used in naming the output columns.\n",
    "    use_bigrams (bool): Whether to tokenize the text into bigrams. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    combined_df (pd.DataFrame): The original DataFrame, with additional columns for the lowercased text, tokenized text, token frequencies, and total tokens.\n",
    "    \"\"\"\n",
    "    if 'lower_text' not in subset_df.columns:\n",
    "        subset_df['lower_text'] = subset_df[text_column].str.lower()\n",
    "\n",
    "    if 'total_tokens' not in subset_df.columns:\n",
    "        subset_df['total_tokens'] = subset_df['lower_text'].str.split(' ').str.len()\n",
    "    \n",
    "    count_df = count_term_frequencies(subset_df, 'lower_text', terms_list)\n",
    "    combined_df = pd.concat([subset_df, count_df], axis=1)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def process_data(file_path: str, text_column: str, date_column: str, terms_list: List, data_origin: str, title: str, term_type: str, term_mapping: bool, joined_term: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Process the data and calculate term frequencies.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        text_column (str): The name of the column containing the text data.\n",
    "        date_column (str): The name of the column containing the date data.\n",
    "        terms_list (List): A list of terms to calculate frequencies for.\n",
    "        data_origin (str): The origin of the data.\n",
    "        title (str): The title of the data.\n",
    "        id_column (str): The name of the column containing the ID data.\n",
    "        term_type (str): The type of term data.\n",
    "        term_mapping (bool): Whether to map term names for formatting variables.\n",
    "        use_bigrams (bool): Whether to tokenize the text into bigrams.\n",
    "        joined_term (str, optional): A term that we've searched for in multiple formats that we want to normalize into one term. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed and calculated DataFrame.\n",
    "    \"\"\"\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Create a subset of the DataFrame that only includes rows where 'full_text' is not null\n",
    "    subset_df = df[df[text_column].notna()]\n",
    "    subset_df = subset_df.reset_index(drop=True)\n",
    "    # Create a 'cleaned_conference_year' column by converting the 'conference_year' column to string and appending \"-01-01\"\n",
    "    subset_df[f\"cleaned_{date_column}\"] = subset_df[date_column].astype(str) + \"-01-01\"\n",
    "\n",
    "    # Convert the 'cleaned_conference_year' column to datetime format\n",
    "    subset_df[f\"cleaned_{date_column}\"] = pd.to_datetime(subset_df[f\"cleaned_{date_column}\"])\n",
    "\n",
    "    original_terms_list = terms_list.copy()\n",
    "    lower_terms_list = [term.lower() for term in terms_list]\n",
    "\n",
    "    # Process the DataFrame to calculate term frequencies and tokenize text\n",
    "    combined_df = generate_word_counts(subset_df, text_column, lower_terms_list)\n",
    "    # If a tool column is provided, create the 'finalized_tool' column\n",
    "    if joined_term:\n",
    "        plural_joined_term = joined_term + \"s\"\n",
    "        combined_df[f'finalized_{joined_term}'] = np.where(combined_df[joined_term].notnull(), combined_df[joined_term], combined_df[plural_joined_term])\n",
    "        combined_df = combined_df.drop(columns=[joined_term, plural_joined_term])\n",
    "        combined_df = combined_df.rename(columns={f\"finalized_{joined_term}\": joined_term})\n",
    "\n",
    "    id_vars_columns = subset_df.columns.tolist()\n",
    "    melted_combined_df = pd.melt(combined_df, id_vars=id_vars_columns, var_name=term_type, value_name='counts')\n",
    "    melted_combined_df['scaled_counts'] = melted_combined_df['counts'] / melted_combined_df['total_tokens']\n",
    "    \n",
    "    # If term mapping is enabled, map the terms\n",
    "    if term_mapping:\n",
    "        # Create a mapping from lowercase tool name to the correct name\n",
    "        term_mapping = {term.lower(): term for term in original_terms_list}\n",
    "        melted_combined_df[term_type] = melted_combined_df[term_type].replace(term_mapping)\n",
    "\n",
    "    # Group the DataFrame by 'cleaned_conference_year' and 'tool' and calculate the sum of 'counts' for each group, then reset the index\n",
    "    summed_df = melted_combined_df.groupby([f\"cleaned_{date_column}\", term_type]).counts.sum().reset_index()\n",
    "\n",
    "    # Group the DataFrame by 'cleaned_conference_year' and 'tool' and calculate the sum of 'scaled_counts' for each group, then reset the index\n",
    "    scaled_df = melted_combined_df.groupby([f\"cleaned_{date_column}\", term_type]).scaled_counts.sum().reset_index()\n",
    "\n",
    "    # Group the DataFrame by 'cleaned_conference_year' and calculate the sum of 'total_tokens' for each group, then reset the index\n",
    "    total_tokens_df = melted_combined_df.groupby([f\"cleaned_{date_column}\", term_type]).total_tokens.sum().reset_index()\n",
    "\n",
    "    # Merge the grouped DataFrames\n",
    "    grouped_df = pd.merge(summed_df, scaled_df, on=[f\"cleaned_{date_column}\", term_type])\n",
    "\n",
    "    # Merge the grouped DataFrame with the total tokens DataFrame\n",
    "    grouped_df = pd.merge(grouped_df, total_tokens_df, on=[f\"cleaned_{date_column}\", term_type])\n",
    "\n",
    "    # Multiply the 'scaled_counts' column by 100\n",
    "    grouped_df.scaled_counts = grouped_df.scaled_counts * 100\n",
    "\n",
    "    # Rename the 'cleaned_conference_year' column to 'date'\n",
    "    grouped_df = grouped_df.rename(columns={f\"cleaned_{date_column}\": \"date\"})\n",
    "\n",
    "    # Add a 'data_origin' column with the value 'Index of DH Conferences'\n",
    "    grouped_df[\"data_origin\"] = data_origin\n",
    "\n",
    "    # Add a 'title' column with the value 'Index of DH Conferences by Weingart et al (2023)'\n",
    "    grouped_df[\"title\"] = title\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_index_conferences_df = process_data(\n",
    "        file_path=f\"../datasets/dh_conferences_works.csv\",\n",
    "        text_column=\"full_text\",\n",
    "        date_column=\"conference_year\",\n",
    "        terms_list=network_tools,\n",
    "        data_origin=\"Index of DH Conferences\",\n",
    "        title=\"Index of DH Conferences by Weingart et al (2023)\",\n",
    "        term_type=\"tools\",\n",
    "        term_mapping=True,\n",
    "        joined_term=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b43ba_row0_col1, #T_b43ba_row1_col1, #T_b43ba_row2_col1, #T_b43ba_row3_col1, #T_b43ba_row4_col1, #T_b43ba_row5_col1, #T_b43ba_row6_col1, #T_b43ba_row7_col1, #T_b43ba_row8_col1, #T_b43ba_row9_col1, #T_b43ba_row10_col1 {\n",
       "  color: red;\n",
       "}\n",
       "#T_b43ba_row11_col1 {\n",
       "  color: blue;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b43ba\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b43ba_level0_col0\" class=\"col_heading level0 col0\" >tools</th>\n",
       "      <th id=\"T_b43ba_level0_col1\" class=\"col_heading level0 col1\" >counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b43ba_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_b43ba_row0_col0\" class=\"data row0 col0\" >Gephi</td>\n",
       "      <td id=\"T_b43ba_row0_col1\" class=\"data row0 col1\" >302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b43ba_level0_row1\" class=\"row_heading level0 row1\" >3</th>\n",
       "      <td id=\"T_b43ba_row1_col0\" class=\"data row1 col0\" >Neo4j</td>\n",
       "      <td id=\"T_b43ba_row1_col1\" class=\"data row1 col1\" >88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b43ba_level0_row2\" class=\"row_heading level0 row2\" >10</th>\n",
       "      <td id=\"T_b43ba_row2_col0\" class=\"data row2 col0\" >nodegoat</td>\n",
       "      <td id=\"T_b43ba_row2_col1\" class=\"data row2 col1\" >82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b43ba_level0_row3\" class=\"row_heading level0 row3\" >7</th>\n",
       "      <td id=\"T_b43ba_row3_col0\" class=\"data row3 col0\" >Palladio</td>\n",
       "      <td id=\"T_b43ba_row3_col1\" class=\"data row3 col1\" >32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b43ba_level0_row4\" class=\"row_heading level0 row4\" >0</th>\n",
       "      <td id=\"T_b43ba_row4_col0\" class=\"data row4 col0\" >Cytoscape</td>\n",
       "      <td id=\"T_b43ba_row4_col1\" class=\"data row4 col1\" >20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b43ba_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_b43ba_row5_col0\" class=\"data row5 col0\" >NetworkX</td>\n",
       "      <td id=\"T_b43ba_row5_col1\" class=\"data row5 col1\" >18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b43ba_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_b43ba_row6_col0\" class=\"data row6 col0\" >NodeXL</td>\n",
       "      <td id=\"T_b43ba_row6_col1\" class=\"data row6 col1\" >16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b43ba_level0_row7\" class=\"row_heading level0 row7\" >9</th>\n",
       "      <td id=\"T_b43ba_row7_col0\" class=\"data row7 col0\" >igraph</td>\n",
       "      <td id=\"T_b43ba_row7_col1\" class=\"data row7 col1\" >15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b43ba_level0_row8\" class=\"row_heading level0 row8\" >2</th>\n",
       "      <td id=\"T_b43ba_row8_col0\" class=\"data row8 col0\" >Graphviz</td>\n",
       "      <td id=\"T_b43ba_row8_col1\" class=\"data row8 col1\" >9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b43ba_level0_row9\" class=\"row_heading level0 row9\" >8</th>\n",
       "      <td id=\"T_b43ba_row9_col0\" class=\"data row9 col0\" >Textexture</td>\n",
       "      <td id=\"T_b43ba_row9_col1\" class=\"data row9 col1\" >7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b43ba_level0_row10\" class=\"row_heading level0 row10\" >11</th>\n",
       "      <td id=\"T_b43ba_row10_col0\" class=\"data row10 col0\" >sigma.js</td>\n",
       "      <td id=\"T_b43ba_row10_col1\" class=\"data row10 col1\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b43ba_level0_row11\" class=\"row_heading level0 row11\" >4</th>\n",
       "      <td id=\"T_b43ba_row11_col0\" class=\"data row11 col0\" >Netlytic</td>\n",
       "      <td id=\"T_b43ba_row11_col1\" class=\"data row11 col1\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16a161ee0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_index_conferences_df.groupby('tools').counts.sum().reset_index().sort_values(by='counts', ascending=False).style.pipe(make_pretty, subset_columns=['counts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final results are above, and are derived using a Python library called `scikit-learn` and the `CountVectorizer` method. This method is very similar to our `string matching`, `lowercasing`, `tokenization` approach (it does all three), but it is much faster and can handle bigrams. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge-cases-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
